ğŸŒ Website-Based AI Chatbot (RAG System)

ğŸ“Œ Project Overview

This project is a website-based AI chatbot that answers user questions strictly based on the content of a given website.

The chatbot works by:
	â€¢	Crawling a website
	â€¢	Cleaning and processing the content
	â€¢	Converting text into embeddings
	â€¢	Storing them in a vector database
	â€¢	Using a local Large Language Model (LLaMA-3) to generate answers only from the retrieved context

If the answer is not present on the website, the chatbot clearly responds that the information is unavailable, avoiding hallucinations.

â¸»

ğŸ¯ Problem Statement

Most AI chatbots provide answers even when the information is not available in the source, which leads to hallucinations and incorrect responses.

This project solves that problem by implementing a Retrieval Augmented Generation (RAG) pipeline that ensures:
	â€¢	Answers are grounded in real website data
	â€¢	No guessing or external knowledge is used
	â€¢	Safe and controlled AI behavior

â¸»

ğŸ§  Key Features
	â€¢	ğŸŒ Website crawling and indexing
	â€¢	ğŸ§¹ HTML content cleaning
	â€¢	âœ‚ï¸ Text chunking with metadata
	â€¢	ğŸ”¢ Embedding generation
	â€¢	ğŸ—„ï¸ Vector storage using ChromaDB
	â€¢	ğŸ” Semantic similarity search
	â€¢	ğŸ¤– Local LLaMA-3 inference via Ollama
	â€¢	ğŸš« Hallucination prevention
	â€¢	ğŸ–¥ï¸ Simple and clean Streamlit UI

â¸»

ğŸ—ï¸ System Architecture
	1.	User enters a website URL
	2.	Website pages are crawled
	3.	HTML content is cleaned
	4.	Text is split into chunks
	5.	Chunks are converted into embeddings
	6.	Embeddings are stored in a vector database
	7.	User asks a question
	8.	Relevant chunks are retrieved using similarity search
	9.	LLaMA-3 generates an answer only from retrieved context
	10.	If no relevant context exists, a safe fallback response is returned

â¸»

ğŸ§° Tech Stack
	â€¢	Frontend: Streamlit
	â€¢	Backend: Python
	â€¢	LLM: LLaMA-3 (local, via Ollama)
	â€¢	Embeddings: Local embedding model
	â€¢	Vector Database: ChromaDB
	â€¢	Frameworks/Libraries:
	â€¢	LangChain
	â€¢	BeautifulSoup
	â€¢	Requests
	â€¢	Streamlit

â¸»

ğŸš€ Local Setup & Execution (Required as per PDF)

ğŸ”¹ Prerequisites
	â€¢	Python 3.10+
	â€¢	Git
	â€¢	Ollama installed on system
ğŸ”¹ Step 1: Clone the Repository
    git clone <your-github-repo-link>
    cd website_chatbot
ğŸ”¹ Step 2: Create Virtual Environment
    python -m venv venv
    source venv/bin/activate   # macOS/Linux
ğŸ”¹ Step 3: Install Dependencies
    pip install -r requirements.txt
ğŸ”¹ Step 4: Install & Run Ollama

Install Ollama from:
https://ollama.com/download

Pull LLaMA-3 model:
                    ollama pull llama3
Verify installation:
                    ollama run llama3
ğŸ”¹ Step 5: Run the Application
    streamlit run app.py
The app will be available at:
                            http://localhost:8501
ğŸ§ª How to Use the Chatbot
	1.	Enter a valid website URL
	2.	Click Index Website
	3.	Wait for indexing to complete
	4.	Ask questions related only to that website
	5.	If the information exists â†’ answer is generated
	6.	If not â†’ chatbot responds safely with a fallback message

ğŸš« Hallucination Control

The chatbot does not answer:
	â€¢	Questions unrelated to the indexed website
	â€¢	General knowledge questions
	â€¢	Personal or speculative queries

Example:
	â€¢	âŒ â€œiPhone 15 priceâ€
	â€¢	âŒ â€œWho is the Prime Minister of India?â€

Response:

â€œThe answer is not available in the indexed website content.â€

This behavior is intentional and correct.

â¸»

ğŸŒ Streamlit Deployment Note (PDF Requirement)

This project uses a local LLaMA-3 model via Ollama.

Due to Streamlit Cloud limitations (no support for system-level binaries or local LLM runtimes), the full AI pipeline cannot run on Streamlit Cloud.

Therefore, the project is designed for local execution, and all local setup steps are clearly documented above, as allowed by the assignment requirements.

â¸»

ğŸ“Œ Why Local LLM (LLaMA-3)?
	â€¢	No dependency on paid APIs
	â€¢	No API key required
	â€¢	Better control over data privacy
	â€¢	Demonstrates real-world AI system design
	â€¢	Avoids vendor lock-in

The LLM layer is swappable, meaning the same architecture can support hosted APIs in production environments if required.

â¸»

ğŸ“„ Conclusion

This project demonstrates a production-ready Retrieval Augmented Generation system with:
	â€¢	Safe AI behavior
	â€¢	Real website grounding
	â€¢	Clear architecture
	â€¢	Practical engineering decisions

It focuses on correctness, explainability, and reliability, which are critical for real-world AI applications.

â¸»

ğŸ‘¨â€ğŸ’» Author

Raju Sewda
B.Tech (CSE/IT)
Website-Based AI Chatbot Project
